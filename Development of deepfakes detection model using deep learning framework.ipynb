{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sys\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.autonotebook import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to Load Video Names and Labels from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class for loading video frames and labels\n",
    "class video_dataset(Dataset):\n",
    "    def __init__(self, video_names, labels, sequence_length=60, transform=None):\n",
    "        self.video_names = video_names  # List of video file names\n",
    "        self.labels = labels  # Corresponding labels \n",
    "        self.transform = transform  # Transformations to apply to the frames\n",
    "        self.count = sequence_length  # Number of frames to extract from each video\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_names)  # Return the number of videos in the dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_names[idx]  # Get the path of the video file\n",
    "        frames = []\n",
    "        a = int(100 / self.count)  # Calculate the range for random starting frame\n",
    "        first_frame = np.random.randint(0, a)  # Randomly select the starting frame\n",
    "        temp_video = video_path.split('/')[-1]  # Extract the video file name\n",
    "        label = lab.loc[lab[\"file\"] == temp_video, \"label\"].values  # Get the label for the video\n",
    "        \n",
    "        # Convert label to numeric value\n",
    "        if label == 'FAKE':\n",
    "            label = 0\n",
    "        if label == 'REAL':\n",
    "            label = 1\n",
    "        \n",
    "        # Extract frames from the video\n",
    "        for i, frame in enumerate(self.frame_extract(video_path)):\n",
    "            frames.append(self.transform(frame))  # Apply transformation to each frame\n",
    "            if len(frames) == self.count:  # Stop when the required number of frames is reached\n",
    "                break\n",
    "        \n",
    "        frames = torch.stack(frames)  # Convert list of frames to a tensor\n",
    "        frames = frames[:self.count]  # Ensure the tensor has the correct number of frames\n",
    "        return frames, label  # Return the frames and label\n",
    "\n",
    "    def frame_extract(self, path):\n",
    "        vidObj = cv2.VideoCapture(path)\n",
    "        success = 1\n",
    "        while success:\n",
    "            success, image = vidObj.read()  # Read a frame from the video\n",
    "            if success:\n",
    "                yield image  # Return the frame to the caller\n",
    "\n",
    "# Function to plot an image tensor\n",
    "def im_plot(tensor):\n",
    "    image = tensor.cpu().numpy().transpose(1, 2, 0)  # Convert tensor to numpy array and reorder dimensions\n",
    "    b, g, r = cv2.split(image) \n",
    "    image = cv2.merge((r, g, b))\n",
    "    image = image * [0.22803, 0.22145, 0.216989] + [0.43216, 0.394666, 0.37645]  # Normalize the image\n",
    "    image = image * 255.0  # Rescale pixel values to 0-255\n",
    "    plt.imshow(image.astype(int)) \n",
    "    plt.show()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to Count Real and Fake Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_real_and_fake_videos(data_list):\n",
    "    header_list = [\"file\", \"label\"]\n",
    "    # Load the CSV file containing video file names and labels\n",
    "    lab = pd.read_csv('/content/drive/MyDrive/file_names.csv', names=header_list)\n",
    "\n",
    "    # Counter \n",
    "    fake = 0  \n",
    "    real = 0 \n",
    "\n",
    "    for files_pattern in data_list:\n",
    "        # Get all file paths that match the pattern\n",
    "        file_paths = glob.glob(files_pattern)\n",
    "        for file_path in file_paths:\n",
    "            temp_video = os.path.basename(file_path)  # Extract the video file name\n",
    "\n",
    "            # Get the label for the video\n",
    "            label = lab.loc[lab[\"file\"] == temp_video, \"label\"].values\n",
    "\n",
    "            # Check if the label exists\n",
    "            if len(label) > 0:\n",
    "                label = label[0]\n",
    "                # Increment the appropriate counter based on the label\n",
    "                if label == 'FAKE':\n",
    "                    fake += 1\n",
    "                elif label == 'REAL':\n",
    "                    real += 1\n",
    "            else:\n",
    "                print(f\"No label found for {temp_video}\")\n",
    "\n",
    "    return real, fake\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to Define and Load Data Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data_transforms():\n",
    "    im_size = 112  # Desired image size (width and height)\n",
    "    # Values for normalization\n",
    "    mean = [0.485, 0.456, 0.406]  \n",
    "    std = [0.229, 0.224, 0.225] \n",
    "\n",
    "    # Define transformations for training data\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.ToPILImage(),  # Convert the image to a PIL Image\n",
    "        transforms.Resize((im_size, im_size)),  # Resize the image to the desired size\n",
    "        transforms.ToTensor(),  # Convert the image to a tensor\n",
    "        transforms.Normalize(mean, std)  # Normalize the image with the specified mean and std\n",
    "    ])\n",
    "\n",
    "    # Define transformations for testing data (same as for training data)\n",
    "    test_transforms = transforms.Compose([\n",
    "        transforms.ToPILImage(), \n",
    "        transforms.Resize((im_size, im_size)), \n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize(mean, std)  \n",
    "    ])\n",
    "\n",
    "    return train_transforms, test_transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Definition with Feature Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom neural network model with feature visualization\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_classes, latent_dim=2048, lstm_layers=1, hidden_dim=1024, bidirectional=False):\n",
    "        super(Model, self).__init__()\n",
    "        # Load a pre-trained ResNeXt-50 model and remove the last two layers (classification head and avg pooling)\n",
    "        model = models.resnext50_32x4d(pretrained=True)\n",
    "        self.model = nn.Sequential(*list(model.children())[:-2])\n",
    "        # Define an LSTM layer to process the feature maps\n",
    "        self.lstm = nn.LSTM(latent_dim, hidden_dim, lstm_layers, bidirectional)\n",
    "        # Define activation function and dropout layer\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.dp = nn.Dropout(0.4)\n",
    "        # Define a fully connected layer to output class scores\n",
    "        self.linear1 = nn.Linear(hidden_dim, num_classes)\n",
    "        # Define an adaptive average pooling layer\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: Input tensor of shape (batch_size, seq_length, channels, height, width)\n",
    "        batch_size, seq_length, c, h, w = x.shape\n",
    "        # Reshape the input tensor to process each frame individually\n",
    "        x = x.view(batch_size * seq_length, c, h, w)\n",
    "        # Extract feature maps using the CNN model\n",
    "        fmap = self.model(x)\n",
    "        # Apply adaptive average pooling\n",
    "        x = self.avgpool(fmap)\n",
    "        # Reshape the tensor to (batch_size, seq_length, latent_dim)\n",
    "        x = x.view(batch_size, seq_length, 2048)\n",
    "        # Pass the reshaped tensor through the LSTM\n",
    "        x_lstm, _ = self.lstm(x, None)\n",
    "        # Return the feature maps and the class scores after applying dropout and the fully connected layer\n",
    "        return fmap, self.dp(self.linear1(torch.mean(x_lstm, dim=1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the Model class with 2 output classes and move it to the GPU\n",
    "model = Model(2).cuda()\n",
    "\n",
    "# Create a dummy input tensor with shape (batch_size, seq_length, channels, height, width) and move it to the GPU\n",
    "a, b = model(torch.from_numpy(np.empty((1, 20, 3, 112, 112))).type(torch.cuda.FloatTensor))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions For Training, Testing and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch, num_epochs, data_loader, model, criterion, optimizer):\n",
    "    model.train()  # Set the model to training mode\n",
    "    losses = AverageMeter() \n",
    "    accuracies = AverageMeter()  \n",
    "    t = []  # Initialize a list to keep track of timestamps \n",
    "\n",
    "    # Loop over batches in the data loader\n",
    "    for i, (inputs, targets) in enumerate(data_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            targets = targets.type(torch.cuda.LongTensor)  # Move targets to GPU and convert to LongTensor\n",
    "            inputs = inputs.cuda()  # Move inputs to GPU\n",
    "        \n",
    "        _, outputs = model(inputs)  # Forward pass through the model\n",
    "        loss = criterion(outputs, targets.type(torch.cuda.LongTensor))  \n",
    "        acc = calculate_accuracy(outputs, targets.type(torch.cuda.LongTensor))  \n",
    "        \n",
    "        # Update loss and accuracy meters\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        accuracies.update(acc, inputs.size(0))\n",
    "        \n",
    "        optimizer.zero_grad()  # Clear the gradients of all optimized tensors\n",
    "        loss.backward()  # Backward pass to compute gradients\n",
    "        optimizer.step()  # Update model parameters\n",
    "        \n",
    "        # Print progress information\n",
    "        sys.stdout.write(\n",
    "            \"\\r[Epoch %d/%d] [Batch %d / %d] [Loss: %f, Acc: %.2f%%]\"\n",
    "            % (\n",
    "                epoch,\n",
    "                num_epochs,\n",
    "                i,\n",
    "                len(data_loader),\n",
    "                losses.avg,\n",
    "                accuracies.avg))\n",
    "    \n",
    "    return losses.avg, accuracies.avg  \n",
    "\n",
    "def test(epoch, model, data_loader, criterion):\n",
    "    print('Testing')\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    losses = AverageMeter()  \n",
    "    accuracies = AverageMeter()  \n",
    "    pred = []  \n",
    "    true = []  \n",
    "    count = 0  \n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        # Loop over batches in the data loader\n",
    "        for i, (inputs, targets) in enumerate(data_loader):\n",
    "            if torch.cuda.is_available():\n",
    "                targets = targets.cuda().type(torch.cuda.FloatTensor)  # Move targets to GPU and convert to FloatTensor\n",
    "                inputs = inputs.cuda()  \n",
    "            \n",
    "            _, outputs = model(inputs)  \n",
    "            loss = torch.mean(criterion(outputs, targets.type(torch.cuda.LongTensor)))  \n",
    "            acc = calculate_accuracy(outputs, targets.type(torch.cuda.LongTensor))  \n",
    "            \n",
    "            # Get predictions and update lists\n",
    "            _, p = torch.max(outputs, 1)\n",
    "            true += (targets.type(torch.cuda.LongTensor)).detach().cpu().numpy().reshape(len(targets)).tolist()\n",
    "            pred += p.detach().cpu().numpy().reshape(len(p)).tolist()\n",
    "            \n",
    "            # Update loss and accuracy meters\n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            accuracies.update(acc, inputs.size(0))\n",
    "            \n",
    "            # Print progress information\n",
    "            sys.stdout.write(\n",
    "                \"\\r[Batch %d / %d] [Loss: %f, Acc: %.2f%%]\"\n",
    "                % (\n",
    "                    i,\n",
    "                    len(data_loader),\n",
    "                    losses.avg,\n",
    "                    accuracies.avg\n",
    "                )\n",
    "            )\n",
    "        print('\\nAccuracy {}'.format(accuracies.avg))  \n",
    "    \n",
    "    return true, pred, losses.avg, accuracies.avg  \n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()  # Initialize the meter\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0  # Current value\n",
    "        self.avg = 0  # Average value\n",
    "        self.sum = 0  # Sum of values\n",
    "        self.count = 0  # Count of values\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val  # Update the current value\n",
    "        self.sum += val * n  # Update the sum\n",
    "        self.count += n  # Update the count\n",
    "        self.avg = self.sum / self.count  # Update the average value\n",
    "\n",
    "def calculate_accuracy(outputs, targets):\n",
    "    batch_size = targets.size(0)  # Get the batch size\n",
    "\n",
    "    _, pred = outputs.topk(1, 1, True)  # Get the top-1 predictions\n",
    "    pred = pred.t()  # Transpose predictions\n",
    "    correct = pred.eq(targets.view(1, -1))  # Compare predictions to targets\n",
    "    n_correct_elems = correct.float().sum().item()  # Count correct predictions\n",
    "    return 100 * n_correct_elems / batch_size  # Return accuracy percentage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to Plot Confusion Matrix, Loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to output and plot the confusion matrix\n",
    "def print_confusion_matrix(y_true, y_pred):\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Print confusion matrix values\n",
    "    print('True positive = ', cm[0][0])\n",
    "    print('False positive = ', cm[0][1])\n",
    "    print('False negative = ', cm[1][0])\n",
    "    print('True negative = ', cm[1][1])\n",
    "    print('\\n')\n",
    "    \n",
    "    # Create a DataFrame for the confusion matrix\n",
    "    df_cm = pd.DataFrame(cm, range(2), range(2))\n",
    "    \n",
    "    # Set font scale for heatmap labels\n",
    "    sn.set(font_scale=1.4)  # for label size\n",
    "    \n",
    "    # Plot heatmap of confusion matrix\n",
    "    sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16})  # font size\n",
    "    plt.ylabel('Actual label', size=20)\n",
    "    plt.xlabel('Predicted label', size=20)\n",
    "    plt.xticks(np.arange(2), ['Fake', 'Real'], size=16)\n",
    "    plt.yticks(np.arange(2), ['Fake', 'Real'], size=16)\n",
    "    plt.ylim([2, 0])  # Invert y-axis for better visualization\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate and print accuracy\n",
    "    calculated_acc = (cm[0][0] + cm[1][1]) / (cm[0][0] + cm[0][1] + cm[1][0] + cm[1][1])\n",
    "    print(\"Calculated Accuracy\", calculated_acc * 100)\n",
    "\n",
    "# Function to plot training and validation loss\n",
    "def plot_loss(train_loss_avg, test_loss_avg, num_epochs):\n",
    "    loss_train = train_loss_avg  # Average training loss\n",
    "    loss_val = test_loss_avg  # Average validation loss\n",
    "    epochs = range(1, num_epochs + 1)  # Epochs range\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    plt.plot(epochs, loss_train, 'g', label='Training loss')\n",
    "    plt.plot(epochs, loss_val, 'b', label='Validation loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Function to plot training and validation accuracy\n",
    "def plot_accuracy(train_accuracy, test_accuracy, num_epochs):\n",
    "    accuracy_train = train_accuracy  # Average training accuracy\n",
    "    accuracy_val = test_accuracy  # Average validation accuracy\n",
    "    epochs = range(1, num_epochs + 1)  # Epochs range\n",
    "    \n",
    "    # Plot training and validation accuracy\n",
    "    plt.plot(epochs, accuracy_train, 'g', label='Training accuracy')\n",
    "    plt.plot(epochs, accuracy_val, 'b', label='Validation accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for Image Conversion and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size = 112\n",
    "mean = [0.485, 0.456, 0.406]  \n",
    "std = [0.229, 0.224, 0.225]   \n",
    "\n",
    "# Initialize Softmax function and inverse normalization transform\n",
    "sm = nn.Softmax()\n",
    "inv_normalize = transforms.Normalize(mean=-1*np.divide(mean, std), std=np.divide([1, 1, 1], std))\n",
    "\n",
    "def im_convert(tensor):\n",
    "    \"\"\" Convert a tensor to an image and save it. \"\"\"\n",
    "    image = tensor.to(\"cpu\").clone().detach()  # Move tensor to CPU and detach\n",
    "    image = image.squeeze()  # Remove single-dimensional entries\n",
    "    image = inv_normalize(image)  # Apply inverse normalization\n",
    "    image = image.numpy()  # Convert tensor to numpy array\n",
    "    image = image.transpose(1, 2, 0)  # Change dimensions from (C, H, W) to (H, W, C)\n",
    "    image = image.clip(0, 1)  # Clip values to [0, 1]\n",
    "    cv2.imwrite('./2.png', image*255)  # Save the image\n",
    "    return image\n",
    "\n",
    "#  Predict the class of an image and visualize the result. \n",
    "def predict(model, img, path='./', confidence_threshold=95.9):\n",
    "    fmap, logits = model(img.to('cuda'))  # Forward pass through the model\n",
    "    params = list(model.parameters())  # Get model parameters\n",
    "    weight_softmax = model.linear1.weight.detach().cpu().numpy()  # Get weights of the final linear layer\n",
    "\n",
    "    logits = sm(logits)  # Apply Softmax to logits\n",
    "    _, prediction = torch.max(logits, 1)  # Get the predicted class\n",
    "    confidence = logits[:, int(prediction.item())].item() * 100  # Calculate confidence score\n",
    "\n",
    "    print('Confidence of prediction:', confidence)\n",
    "\n",
    "    # Generate heatmap from feature maps\n",
    "    idx = np.argmax(logits.detach().cpu().numpy())\n",
    "    bz, nc, h, w = fmap.shape\n",
    "    out = np.dot(fmap[-1].detach().cpu().numpy().reshape((nc, h * w)).T,\n",
    "                     weight_softmax[:, :].reshape((nc, -1)))\n",
    "    predict = out.reshape(h, w)\n",
    "    predict = predict - np.min(predict)  # Normalize prediction\n",
    "    predict_img = predict / np.max(predict)\n",
    "    predict_img = np.uint8(255 * predict_img)\n",
    "    out = cv2.resize(predict_img, (im_size, im_size))  # Resize heatmap\n",
    "    heatmap = cv2.applyColorMap(out, cv2.COLORMAP_JET)  # Apply color map\n",
    "\n",
    "    # Convert image and save result\n",
    "    img = im_convert(img[:, -1, :, :, :])\n",
    "    result = heatmap * 0.5 + img * 0.8 * 255\n",
    "    cv2.imwrite('/content/1.png', result)\n",
    "    result1 = heatmap * 0.5 / 255 + img * 0.8\n",
    "    r, g, b = cv2.split(result1)  # Split channels\n",
    "    result1 = cv2.merge((r, g, b))  # Merge channels\n",
    "    plt.imshow(result1)\n",
    "    plt.show()\n",
    "\n",
    "    # Check if confidence is above the threshold\n",
    "    if confidence >= confidence_threshold:\n",
    "        return [int(prediction.item()), confidence] \n",
    "    else:\n",
    "        return [1, confidence] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Defination For Validation Dataset Creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationDataset(Dataset):\n",
    "    \n",
    "    #Initialize the dataset with video names, sequence length, and optional transformations.\n",
    "    def __init__(self, video_names, sequence_length=60, transform=None):\n",
    "    \n",
    "        self.video_names = video_names  # List of paths to video files.\n",
    "        self.transform = transform  # Number of frames to sample from each video.\n",
    "        self.count = sequence_length\n",
    "\n",
    "    # Return the number of videos in the dataset.\n",
    "    def __len__(self):\n",
    "     \n",
    "        return len(self.video_names)\n",
    "    \n",
    "    # Retrieve a sample from the dataset at the specified index.\n",
    "def __getitem__(self, idx):\n",
    "    video_path = self.video_names[idx]  # Get the video path at index `idx`\n",
    "    frames = []\n",
    "    \n",
    "    a = int(100 / self.count)\n",
    "    first_frame = np.random.randint(0, a)  # Randomly select a starting frame\n",
    "\n",
    "    # Initialize the MediaPipe Face Detection model\n",
    "    mp_face_detection = mp.solutions.face_detection\n",
    "    detector = mp_face_detection.FaceDetection(min_detection_confidence=0.5)\n",
    "\n",
    "    for i, frame in enumerate(self.frame_extract(video_path)):\n",
    "        # Convert the frame to RGB as MediaPipe expects RGB format\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Detect faces using MediaPipe\n",
    "        results = detector.process(rgb_frame)\n",
    "        \n",
    "        if results.detections:\n",
    "            # Extract the first detected face bounding box\n",
    "            detection = results.detections[0]\n",
    "            bboxC = detection.location_data.relative_bounding_box\n",
    "\n",
    "            ih, iw, _ = frame.shape\n",
    "            x, y, w, h = int(bboxC.xmin * iw), int(bboxC.ymin * ih), \\\n",
    "                         int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "            frame = frame[y:y + h, x:x + w, :]\n",
    "        \n",
    "        # Apply any transformation, if provided\n",
    "        if self.transform is not None:\n",
    "            frame = self.transform(frame)\n",
    "        \n",
    "        frames.append(frame)\n",
    "        \n",
    "        # Stop after collecting the required number of frames\n",
    "        if len(frames) == self.count:\n",
    "            break\n",
    "    \n",
    "    # Stack frames into a tensor and add a batch dimension\n",
    "    frames = torch.stack(frames)\n",
    "    frames = frames[:self.count]\n",
    "    \n",
    "    return frames.unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "\n",
    "    # Generator function to yield frames from a video file.\n",
    "    def frame_extract(self, path):\n",
    "          \n",
    "        vidObj = cv2.VideoCapture(path)  # Open video file\n",
    "        success = True\n",
    "        while success:\n",
    "            success, image = vidObj.read()  # Read a frame from the video\n",
    "            if success:\n",
    "                yield image  # Yield the frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Video Data and Creating Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Retrieve a list of face-only video file paths \n",
    "video_files = glob.glob('/content/drive/MyDrive/Face_only_Videos/*.mp4')\n",
    "random.shuffle(video_files)\n",
    "\n",
    "\n",
    "# Load labels from CSV file into a DataFrame\n",
    "header_list = [\"file\", \"label\"]\n",
    "labels = pd.read_csv('/content/drive/MyDrive/file_names.csv', names=header_list)\n",
    "lab = labels\n",
    "\n",
    "train_videos, valid_videos = train_test_split(video_files, test_size=0.3)\n",
    "\n",
    "# Print the number of real and fake videos in both the training and validation sets\n",
    "print(\"TRAIN: \", \"Real:\", number_of_real_and_fake_videos(train_videos)[0], \" Fake:\", number_of_real_and_fake_videos(train_videos)[1])\n",
    "print(\"TEST: \", \"Real:\", number_of_real_and_fake_videos(valid_videos)[0], \" Fake:\", number_of_real_and_fake_videos(valid_videos)[1])\n",
    "\n",
    "# Get data transformations for training and testing\n",
    "train_transforms, test_transforms = get_data_transforms()\n",
    "\n",
    "# Create dataset objects for training and validation data\n",
    "train_data = video_dataset(train_videos, labels, sequence_length=10, transform=train_transforms)\n",
    "val_data = video_dataset(valid_videos, labels, sequence_length=10, transform=test_transforms)\n",
    "\n",
    "# Create DataLoader objects for training and validation datasets\n",
    "train_loader = DataLoader(train_data, batch_size=4, shuffle=True, num_workers=4)\n",
    "valid_loader = DataLoader(val_data, batch_size=4, shuffle=False, num_workers=4)\n",
    "\n",
    "# Retrieve an example image and label from the training dataset\n",
    "image, label = train_data[0]\n",
    "\n",
    "# Plot the first frame of the retrieved example image\n",
    "im_plot(image[0, :, :, :])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Evaluation of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr = 1e-7  # learning rate \n",
    "num_epochs = 30\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "# Define the loss function and move it to GPU if available\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "# Initialize lists to store average training and testing losses and accuracies\n",
    "train_loss_avg = []\n",
    "train_accuracy = []\n",
    "test_loss_avg = []\n",
    "test_accuracy = []\n",
    "\n",
    "try:\n",
    "    # Training loop for the specified number of epochs\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        l, acc = train_epoch(epoch, num_epochs, train_loader, model, criterion, optimizer)\n",
    "        train_loss_avg.append(l)\n",
    "        train_accuracy.append(acc)\n",
    "        \n",
    "        # Evaluate model performance on validation set\n",
    "        true, pred, tl, t_acc = test(epoch, model, valid_loader, criterion)\n",
    "        test_loss_avg.append(tl)\n",
    "        test_accuracy.append(t_acc)\n",
    "\n",
    "except RuntimeError as e:\n",
    "    if \"NaN\" in str(e):\n",
    "        print(\"NaN detected during forward pass. Investigate further.\")\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "# Save the trained model's weights\n",
    "torch.save(model.state_dict(), '/content/checkpoint.pt')\n",
    "\n",
    "plot_loss(train_loss_avg, test_loss_avg, len(train_loss_avg))\n",
    "plot_accuracy(train_accuracy, test_accuracy, len(train_accuracy))\n",
    "print(confusion_matrix(true, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classify the videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_transforms, test_transforms = get_data_transforms()\n",
    "\n",
    "# Define the path to the video file(s) to be processed\n",
    "path_to_videos = [\"/content/video.mp4\"]\n",
    "\n",
    "# Create a ValidationDataset instance\n",
    "video_dataset = ValidationDataset(path_to_videos, sequence_length=20, transform=train_transforms)\n",
    "model = Model(2).cuda()\n",
    "\n",
    "# Load the pre-trained model's weights\n",
    "path_to_model = '/content/drive/MyDrive/checkpoint.pt'\n",
    "model.load_state_dict(torch.load(path_to_model))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "for i in range(len(path_to_videos)):\n",
    "    print(path_to_videos[i])  \n",
    "    # Make a prediction using the model and the video data\n",
    "    prediction = predict(model, video_dataset[i], './')\n",
    "    \n",
    "    # Print the result based on the prediction\n",
    "    if prediction[0] == 1:\n",
    "        print(\"REAL\")\n",
    "    else:\n",
    "        print(\"FAKE\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
